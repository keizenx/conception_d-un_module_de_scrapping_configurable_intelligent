# ğŸš€ Scraper Pro - Module de Scraping Configurable & Intelligent

> Plateforme complÃ¨te de web scraping avec intelligence artificielle, analyse automatique et notifications en temps rÃ©el.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Django](https://img.shields.io/badge/Django-5.0+-green.svg)](https://www.djangoproject.com/)
[![React](https://img.shields.io/badge/React-18.3+-61DAFB.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Table des MatiÃ¨res

- [AperÃ§u](#-aperÃ§u)
- [FonctionnalitÃ©s Principales](#-fonctionnalitÃ©s-principales)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [Technologies](#-technologies)
- [API Documentation](#-api-documentation)
- [Contribution](#-contribution)

## ğŸ¯ AperÃ§u

**Scraper Pro** est une solution complÃ¨te de web scraping intelligent qui permet d'extraire, analyser et exporter des donnÃ©es web de maniÃ¨re automatisÃ©e et configurable. Avec son interface moderne et intuitive, il offre une expÃ©rience utilisateur optimale pour les professionnels du data scraping.

### âœ¨ Points Forts

- ğŸ¤– **Analyse Intelligente** - DÃ©tection automatique du contenu scrapable
- âš¡ **Mode Asynchrone** - Navigation libre pendant le scraping
- ğŸ”” **Notifications Temps RÃ©el** - Son + notifications browser
- ğŸ“Š **Multi-Format Export** - CSV, Excel, JSON, XML, PDF, ZIP Images
- ğŸ¨ **Interface Moderne** - Design dark mode Ã©lÃ©gant
- ğŸ” **Authentification ComplÃ¨te** - SystÃ¨me de login/register sÃ©curisÃ©
- ğŸ“ˆ **Dashboard Analytique** - Visualisation des sessions et statistiques
- ğŸ¯ **SÃ©lection PersonnalisÃ©e** - Choix prÃ©cis des Ã©lÃ©ments Ã  exporter

## ğŸ‘¥ L'Ã‰quipe

*   **Front-end :** Koffi Ornella (Dev), Kouame Aka Richard (PO)
*   **Back-end :** Oumar Vivien (Dev), Kouakou Jean Raphael (Dev)
*   **Gestion :** Beleley Franck (Scrum Master)

## ğŸŒŸ FonctionnalitÃ©s Principales

### 1. Analyse Automatique du Site
- âœ… DÃ©tection automatique des types de contenu (titres, paragraphes, images, liens, etc.)
- âœ… Estimation du nombre de pages
- âœ… VÃ©rification de l'accessibilitÃ© et protection anti-scraping
- âœ… DÃ©tection de la stack technologique
- âœ… DÃ©couverte des sous-domaines scrapables

### 2. Scraping Configurable
- âœ… SÃ©lection des types de contenu Ã  extraire
- âœ… Profondeur de crawling configurable
- âœ… DÃ©lai entre les requÃªtes personnalisable
- âœ… User-Agent customisable
- âœ… SÃ©lecteurs CSS personnalisÃ©s
- âœ… Mode avec/sans sous-domaines

### 3. Export Multi-Format
- ğŸ“„ **CSV** - Format tabulaire standard
- ğŸ“Š **Excel** - Fichier .xlsx avec formatage
- ğŸ“‹ **JSON** - Structure de donnÃ©es complÃ¨te
- ğŸ”– **XML** - Format structurÃ©
- ğŸ“• **PDF** - Rapport professionnel avec images
- ğŸ–¼ï¸ **ZIP Images** - Archive de toutes les images extraites

### 4. Notifications & Mode Asynchrone
- ğŸ”” Notification sonore Ã  la fin du scraping
- ğŸ’¬ Notifications browser (mÃªme hors de l'app)
- ğŸ“Š Barre de progression persistante
- ğŸŒ Navigation libre pendant le traitement
- â° Mises Ã  jour en temps rÃ©el

### 5. SÃ©lection d'Export AvancÃ©e
- ğŸšï¸ Slider pour choisir le nombre d'Ã©lÃ©ments
- â˜‘ï¸ SÃ©lection manuelle d'Ã©lÃ©ments spÃ©cifiques
- ğŸ“ˆ AperÃ§u du nombre d'Ã©lÃ©ments Ã  exporter
- ğŸ¯ Export de tous les sous-Ã©lÃ©ments (plus de "... et X autres")

## ğŸ—ï¸ Architecture

```
scraper-pro/
â”œâ”€â”€ backend/                 # Django REST API
â”‚   â”œâ”€â”€ api/                # Endpoints REST
â”‚   â”‚   â”œâ”€â”€ models.py      # ModÃ¨les de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ serializers.py # SÃ©rialiseurs DRF
â”‚   â”‚   â”œâ”€â”€ views.py       # ViewSets et actions
â”‚   â”‚   â””â”€â”€ urls.py        # Routing API
â”‚   â”œâ”€â”€ config/            # Configuration Django
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ core/          # Logique mÃ©tier
â”‚   â”‚   â”‚   â”œâ”€â”€ analyzer.py          # Analyse de sites
â”‚   â”‚   â”‚   â”œâ”€â”€ scraper.py           # Moteur de scraping
â”‚   â”‚   â”‚   â”œâ”€â”€ fetcher.py           # RÃ©cupÃ©ration de pages
â”‚   â”‚   â”‚   â”œâ”€â”€ content_detector.py  # DÃ©tection de contenu
â”‚   â”‚   â”‚   â”œâ”€â”€ subdomain_finder.py  # DÃ©couverte sous-domaines
â”‚   â”‚   â”‚   â””â”€â”€ path_finder.py       # Crawling de chemins
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ routes/    # Routes API organisÃ©es
â”‚   â””â”€â”€ tests/             # Tests unitaires et d'intÃ©gration
â”‚
â”œâ”€â”€ frontend/              # React + Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ assets/       # CSS et ressources
â”‚   â”‚   â”‚   â””â”€â”€ css/     # Styles par page
â”‚   â”‚   â”œâ”€â”€ components/   # Composants rÃ©utilisables
â”‚   â”‚   â”‚   â”œâ”€â”€ NotificationCenter/  # SystÃ¨me de notifications
â”‚   â”‚   â”‚   â”œâ”€â”€ ProgressLogs/        # Logs de progression
â”‚   â”‚   â”‚   â””â”€â”€ ContentSelector/     # SÃ©lecteur de contenu
â”‚   â”‚   â”œâ”€â”€ contexts/     # React Context API
â”‚   â”‚   â”‚   â”œâ”€â”€ AuthContext.jsx      # Authentification
â”‚   â”‚   â”‚   â””â”€â”€ ScrapingContext.jsx  # Ã‰tat de scraping async
â”‚   â”‚   â”œâ”€â”€ pages/        # Pages de l'application
â”‚   â”‚   â”‚   â”œâ”€â”€ Landing/     # Page d'accueil
â”‚   â”‚   â”‚   â”œâ”€â”€ Login/       # Authentification
â”‚   â”‚   â”‚   â”œâ”€â”€ Register/    # Inscription
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard/   # Tableau de bord
â”‚   â”‚   â”‚   â”œâ”€â”€ Analysis/    # Configuration du scraping
â”‚   â”‚   â”‚   â”œâ”€â”€ Results/     # RÃ©sultats et export
â”‚   â”‚   â”‚   â”œâ”€â”€ Reports/     # Rapports et statistiques
â”‚   â”‚   â”‚   â””â”€â”€ Settings/    # ParamÃ¨tres utilisateur
â”‚   â”‚   â””â”€â”€ services/     # API client
â”‚   â”‚       â””â”€â”€ api.js    # Wrapper axios pour API
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ sounds/       # Sons de notification
â”‚
â””â”€â”€ README.md             # Documentation principale
```

## ğŸš€ Installation

### PrÃ©requis

- **Python 3.11+**
- **Node.js 18+**
- **npm ou yarn**
- **Git**

### 1. Cloner le Repository

```bash
git clone https://github.com/keizenx/conception_d-un_module_de_scrapping_configurable_intelligent.git
cd scraper-pro
```

### 2. Installation Backend (Django)

```bash
cd backend

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# CrÃ©er la base de donnÃ©es
python manage.py migrate

# CrÃ©er un superuser (optionnel)
python manage.py createsuperuser

# Lancer le serveur
python manage.py runserver
```

Le backend sera accessible sur `http://localhost:8000`

### 3. Installation Frontend (React)

```bash
cd frontend

# Installer les dÃ©pendances
npm install

# Lancer le serveur de dÃ©veloppement
npm run dev
```

Le frontend sera accessible sur `http://localhost:5173`

### 4. Build Production

```bash
# Frontend
cd frontend
npm run build

# Les fichiers de production seront dans /dist
```

## âš™ï¸ Configuration

### Variables d'Environnement Backend

CrÃ©er un fichier `.env` dans `/backend`:

```env
# Django
SECRET_KEY=your-secret-key-here
DEBUG=True
ALLOWED_HOSTS=localhost,127.0.0.1

# Database (par dÃ©faut SQLite)
DATABASE_URL=sqlite:///db.sqlite3

# CORS (pour le dÃ©veloppement)
CORS_ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# Scraping
DEFAULT_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
MAX_DEPTH=3
DEFAULT_DELAY=500
REQUEST_TIMEOUT=30
```

### Configuration Frontend

Le fichier `/frontend/src/services/api.js` contient la configuration de l'API:

```javascript
const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://localhost:8000/api';
```

Vous pouvez crÃ©er un fichier `.env` dans `/frontend`:

```env
VITE_API_URL=http://localhost:8000/api
```

## ğŸ“– Utilisation

### 1. Lancer une Analyse

1. AccÃ©dez Ã  la page **Analysis** (`/analysis`)
2. Entrez l'URL du site Ã  analyser (ex: `https://example.com`)
3. Activez/dÃ©sactivez l'inclusion des sous-domaines
4. Cliquez sur **"Analyser le site"**
5. L'analyse se fait en arriÃ¨re-plan - vous pouvez naviguer librement
6. Une notification vous prÃ©viendra quand c'est terminÃ©

### 2. Configurer le Scraping

1. AprÃ¨s l'analyse, sÃ©lectionnez les types de contenu Ã  extraire :
   - ğŸ“Œ Titres principaux
   - ğŸ“ Paragraphes
   - ğŸ”— Liens
   - ğŸ–¼ï¸ Images
   - ğŸ“‹ Listes
   - etc.

2. Configurez les options avancÃ©es :
   - Profondeur de crawling (1-5)
   - DÃ©lai entre requÃªtes (ms)
   - User-Agent personnalisÃ©
   - Timeout des requÃªtes

3. Ajoutez des sÃ©lecteurs CSS personnalisÃ©s (optionnel)

4. Cliquez sur **"Lancer le Scraping"**

### 3. Visualiser les RÃ©sultats

1. La page **Results** (`/results`) affiche tous les Ã©lÃ©ments extraits
2. Utilisez les filtres pour affiner la recherche
3. PrÃ©visualisez les Ã©lÃ©ments en cliquant dessus
4. SÃ©lectionnez des Ã©lÃ©ments spÃ©cifiques (checkboxes)

### 4. Exporter les DonnÃ©es

1. Cliquez sur un bouton d'export (CSV, Excel, JSON, PDF, etc.)
2. Un modal s'ouvre pour configurer l'export :
   - ğŸšï¸ Utilisez le slider pour choisir le nombre d'Ã©lÃ©ments
   - â˜‘ï¸ Ou cochez "Exporter uniquement les sÃ©lectionnÃ©s"
3. Cliquez sur **"Exporter"**
4. Le fichier est tÃ©lÃ©chargÃ© automatiquement

### 5. Consulter le Dashboard

- **Sessions rÃ©centes** avec statut
- **Statistiques** (total sessions, Ã©lÃ©ments extraits, etc.)
- **Bouton "RÃ©sultats"** pour accÃ¨s rapide
- **Actions rapides** (Nouvelle analyse, Rapports)

## ğŸ› ï¸ Technologies

### Backend
- **Django 5.0** - Framework web Python
- **Django REST Framework** - API REST
- **BeautifulSoup4** - Parsing HTML
- **Requests** - RequÃªtes HTTP
- **Playwright** (optionnel) - Navigation browser automatisÃ©e
- **lxml** - Parsing XML rapide

### Frontend
- **React 18.3** - Framework UI
- **Vite** - Build tool ultra-rapide
- **React Router v6** - Navigation
- **jsPDF** - GÃ©nÃ©ration PDF client-side
- **xlsx** - Export Excel

### Base de DonnÃ©es
- **SQLite** (dÃ©veloppement)
- **PostgreSQL** (production recommandÃ©e)

## ğŸ“¡ API Documentation

### Endpoints Principaux

#### Authentification
```http
POST /api/auth/register/
POST /api/auth/login/
POST /api/auth/logout/
GET  /api/auth/user/
```

#### Analyse
```http
POST /api/analyze/
GET  /api/scraping/{id}/logs/
GET  /api/scraping/{id}/status/
GET  /api/scraping/{id}/results/
```

#### Scraping
```http
POST /api/scraping/start/
GET  /api/scraping/{id}/
GET  /api/scraping/
```

#### RÃ©sultats
```http
GET  /api/results/
GET  /api/results/{id}/
GET  /api/results/{id}/export/?type=csv&limit=100&item_ids=1,2,3
```

#### Dashboard
```http
GET  /api/dashboard/stats/
GET  /api/dashboard/recent_sessions/
GET  /api/dashboard/charts/
```

#### Rapports
```http
GET  /api/reports/
GET  /api/reports/stats/
```

### Exemple d'Appel API

```javascript
// Lancer une analyse
const response = await fetch('http://localhost:8000/api/analyze/', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Token your-auth-token'
  },
  body: JSON.stringify({
    url: 'https://example.com',
    include_subdomains: false
  })
});

const data = await response.json();
console.log('Session ID:', data.session_id);
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voici comment contribuer :

1. **Fork** le projet
2. CrÃ©ez votre **branche feature** (`git checkout -b feature/AmazingFeature`)
3. **Committez** vos changements (`git commit -m 'Add some AmazingFeature'`)
4. **Push** vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une **Pull Request**

### Guidelines de Contribution

- Code propre et documentÃ©
- Tests unitaires pour les nouvelles fonctionnalitÃ©s
- Respecter les conventions de nommage
- Commentaires en franÃ§ais pour la cohÃ©rence

## ğŸ“ Roadmap

- [ ] Support OAuth2 (Google, GitHub)
- [ ] Scraping planifiÃ© (cron jobs)
- [ ] API GraphQL
- [ ] Mode headless avec Playwright
- [ ] Support proxy rotation
- [ ] Machine Learning pour dÃ©tection avancÃ©e
- [ ] Exports vers bases de donnÃ©es externes
- [ ] Webhooks pour notifications externes
- [ ] Mode collaboratif multi-utilisateurs

## ğŸ“„ License

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¨â€ğŸ’» Auteur

**Keizenx** - [GitHub](https://github.com/keizenx)

## ğŸ™ Remerciements

- **Django** et **DRF** pour le backend robuste
- **React** et **Vite** pour l'interface moderne
- **BeautifulSoup** pour le parsing HTML efficace
- CommunautÃ© open-source pour les libraries utilisÃ©es

---

<div align="center">
  <strong>â­ Si ce projet vous a aidÃ©, n'hÃ©sitez pas Ã  lui donner une Ã©toile ! â­</strong>
</div>
